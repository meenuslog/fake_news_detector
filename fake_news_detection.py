# -*- coding: utf-8 -*-
"""Task_3:Fake_News_Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HtweQxexEa72EssvfgrkbUeUT8_GeCP0
"""

# Commented out IPython magic to ensure Python compatibility.
import os
import textwrap
from getpass import getpass

# GitHub repo details
username = "meenuslog"
repo_name = "fake_news_detector"
token = getpass("Enter your GitHub token: ")

# Clone repo
repo_url = f"https://{username}:{token}@github.com/{username}/{repo_name}.git"
!git clone $repo_url

# Path to local repo
repo_path = f"/content/{repo_name}"
os.makedirs(f"{repo_path}/data", exist_ok=True)

# Move dataset into the repo's data directory
!cp "/content/News_dataset.zip" "{repo_path}/data/News_dataset.zip"

# Write README.md
readme_content = textwrap.dedent("""
# Fake News Detection

This repository contains code and data for detecting fake news.

## ðŸ“¥ Dataset

Download manually from [Google Drive](https://drive.google.com/file/d/18XKG2zkdLeyW6NkYiuKI0u2mVPNwZ4yE/view?usp=sharing)

    ```
""")

with open(f"{repo_path}/README.md", "w") as f:
    f.write(readme_content)

# Git config and commit
# %cd {repo_path}
!git config --global user.email "emaan.yawer.19@gmail.com"
!git config --global user.name "meenuslog"

!git add .
!git commit -m "Initial commit with README and dataset included"
!git push origin main

"""### Download and unzip dataset, check files"""

!wget https://raw.githubusercontent.com/meenuslog/fake_news_detector/refs/heads/main/data/News_dataset.zip -O News_dataset.zip

# Unzip the downloaded file
import zipfile

with zipfile.ZipFile("News_dataset.zip", "r") as zip_ref:
    zip_ref.extractall("News Dataset")

# Check the contents
import os
print(os.listdir("News Dataset"))

"""###Imports"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, f1_score, classification_report

# Download NLTK resources
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt')

"""###Load and Inspect Dataset"""

# Load Fake and True news CSVs
fake_df = pd.read_csv("/content/fake_news_detector/News Dataset/News _dataset/Fake.csv")
true_df = pd.read_csv("/content/fake_news_detector/News Dataset/News _dataset/True.csv")

# Assign labels: Fake = 0, Real = 1
fake_df['label'] = 0
true_df['label'] = 1

# Combine into a single DataFrame
df = pd.concat([fake_df, true_df], ignore_index=True)

print("Dataset shape:", df.shape)
print(df['label'].value_counts())
print(df.head())

"""###Preprocess Text (Title + Text)"""

import nltk
nltk.download('punkt')
nltk.download('punkt_tab')
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def preprocess(text):
    tokens = nltk.word_tokenize(str(text).lower())  # Lowercase + tokenize
    tokens = [lemmatizer.lemmatize(word) for word in tokens if word.isalpha() and word not in stop_words]
    return ' '.join(tokens)

# title + text into one column
df['content'] = (df['title'] + " " + df['text']).apply(preprocess)
print(df['content'].head())

"""###Split Data into Train and Test Sets"""

X = df['content']
y = df['label']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print("Training samples:", X_train.shape[0])
print("Testing samples:", X_test.shape[0])

"""###Vectorize Text Using TF-IDF"""

tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1,2))
X_train_tfidf = tfidf.fit_transform(X_train)
X_test_tfidf = tfidf.transform(X_test)

print("TF-IDF matrix shape:", X_train_tfidf.shape)

"""##Step 5: Train Classifier

###Option 1: Logistic Regression
"""

lr = LogisticRegression(max_iter=1000)
lr.fit(X_train_tfidf, y_train)
y_pred_lr = lr.predict(X_test_tfidf)

print("=== Logistic Regression ===")
print("Accuracy:", accuracy_score(y_test, y_pred_lr))
print("F1-Score:", f1_score(y_test, y_pred_lr))
print(classification_report(y_test, y_pred_lr))

"""###SVM (Optional)"""

svm = SVC(kernel='linear')
svm.fit(X_train_tfidf, y_train)
y_pred_svm = svm.predict(X_test_tfidf)

print("=== SVM ===")
print("Accuracy:", accuracy_score(y_test, y_pred_svm))
print("F1-Score:", f1_score(y_test, y_pred_svm))
print(classification_report(y_test, y_pred_svm))

"""###Word Cloud Visualization"""

fake_text = ' '.join(df[df['label']==0]['content'])
real_text = ' '.join(df[df['label']==1]['content'])

plt.figure(figsize=(14,6))

# Fake News Word Cloud
plt.subplot(1,2,1)
wordcloud_fake = WordCloud(width=700, height=400, background_color='white').generate(fake_text)
plt.imshow(wordcloud_fake, interpolation='bilinear')
plt.axis('off')
plt.title("Fake News Word Cloud", fontsize=16)

# Real News Word Cloud
plt.subplot(1,2,2)
wordcloud_real = WordCloud(width=700, height=400, background_color='white').generate(real_text)
plt.imshow(wordcloud_real, interpolation='bilinear')
plt.axis('off')
plt.title("Real News Word Cloud", fontsize=16)

plt.show()

"""###Top Words Influencing Fake vs Real News

"""

# Get feature names (words) from TF-IDF
feature_names = tfidf.get_feature_names_out()

# Get coefficients from Logistic Regression
coefficients = lr.coef_[0]  # Shape = (num_features,)

# Create a DataFrame for words and their coefficients
coef_df = pd.DataFrame({
    'word': feature_names,
    'coefficient': coefficients
})

# Top 20 words predicting Fake News (most negative coefficients)
top_fake_words = coef_df.sort_values(by='coefficient').head(20)

# Top 20 words predicting Real News (most positive coefficients)
top_real_words = coef_df.sort_values(by='coefficient', ascending=False).head(20)

print("=== Top 20 Words Predicting Fake News ===")
print(top_fake_words)

print("\n=== Top 20 Words Predicting Real News ===")
print(top_real_words)

"""###Visualize Top Words"""

plt.figure(figsize=(12,5))

# Fake News Words
plt.subplot(1,2,1)
plt.barh(top_fake_words['word'], top_fake_words['coefficient'], color='red')
plt.gca().invert_yaxis()
plt.title("Top Words Predicting Fake News")

# Real News Words
plt.subplot(1,2,2)
plt.barh(top_real_words['word'], top_real_words['coefficient'], color='green')
plt.gca().invert_yaxis()
plt.title("Top Words Predicting Real News")

plt.tight_layout()
plt.show()